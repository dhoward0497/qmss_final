{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "from gensim.models import ldamodel\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.test.utils import common_corpus\n",
    "from gensim.test.utils import common_texts\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from statistics import mean\n",
    "import gensim\n",
    "import math\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "overview = pd.read_csv(\"debate_data/data_overviews/overview_1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def string_2_row(df_w_col_name):\n",
    "    count = 0\n",
    "    for row in df_w_col_name:\n",
    "        df_w_col_name[count] = literal_eval(row)\n",
    "        count+=1\n",
    "        \n",
    "string_2_row(overview[\"moderator\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create stemmer and lemmatizer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of stop words\n",
    "removed_words = [\"ok\",\"yes\",\"right\",\"good\",\"great\",\"hi\",\"look\",\"evening\",\"well\",\"activ\",\"except\",\\\n",
    "                \"think\",\"differ\",\"style\",\"whatev\",\"go\",\"comment\",\"single\",\"sure\",\"no\"]\n",
    "nltk_stop = list(set(stopwords.words('english')))\n",
    "political_stop_words = [\"president\", \"american\", \"america\", \"govern\", \"government\", \"campaign\",\"secretary\",\\\n",
    "                        \"country\",\"applause\",\"question\",\"governor\", \"congresswoman\", \"congress\",\\\n",
    "                       \"congressman\", \"partisan\", \"republican\", \"democrat\", \"DNC\",\"GOP\",\"liberal\",\\\n",
    "                       \"conservative\", \"senate\", \"state\", \"left\",\"right\", \"nomination\",\"parti\",\"party\",\\\n",
    "                       \"democraci\",\"constitut\",\"politician\",\"political\",\"politics\",\"pols\",\"politi\", \"secretari\"]\n",
    "states = ['Alabama', 'Alaska', 'Arizona', 'Arkansas', 'California', 'Colorado', 'Connecticut', \\\n",
    "          'Delaware', 'District of Columbia', 'Florida', 'Georgia', 'Hawaii', 'Idaho', 'Illinois', \\\n",
    "          'Indiana', 'Iowa', 'Kansas', 'Kentucky', 'Louisiana', 'Maine', 'Montana', 'Nebraska', 'Nevada', \\\n",
    "          'New Hampshire', 'New Jersey', 'New Mexico', 'New York', 'North Carolina', 'North Dakota', 'Ohio', \\\n",
    "          'Oklahoma', 'Oregon', 'Maryland', 'Massachusetts', 'Michigan', 'Minnesota', 'Mississippi', 'Missouri',\\\n",
    "          'Pennsylvania', 'Rhode Island', 'South Carolina', 'South Dakota', 'Tennessee', 'Texas', 'Utah', \n",
    "          'Vermont', 'Virginia', 'Washington', 'West Virginia', 'Wisconsin', 'Wyoming']\n",
    "\n",
    "splitted_states = []\n",
    "for s in states:\n",
    "    for x in s.split(\" \"):\n",
    "        splitted_states.append(x)\n",
    "        \n",
    "words = list(set(removed_words + nltk_stop + political_stop_words + splitted_states))\n",
    "stop_words = list(set(words + [x.upper() for x in words] + [x.capitalize() for x in words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# renaming the debates with coder from overview sheet\n",
    "\n",
    "directory = \"debate_data/debates_raw\"\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    \n",
    "    if filename.endswith(\".csv\"):\n",
    "        this_file = pd.read_csv(directory+\"/\"+filename, encoding = \"ISO-8859-1\")\n",
    "        debate_date = (filename.split(\"-\"))[0]        \n",
    "\n",
    "        coder = overview.loc[overview[\"date\"] == debate_date, \"coder\"].iloc[0]\n",
    "\n",
    "        new_name = \"debate_data/debates_clean/\"+ str(coder) +\".csv\"\n",
    "        this_file.to_csv(new_name,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple steps for cleaning debate data, done below, with output of final cleaned CSV\n",
    "\n",
    "directory = \"debate_data/debates_clean\"\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "        \n",
    "    if filename.endswith(\".csv\"):\n",
    "        \n",
    "        # reading file\n",
    "        this_file = pd.read_csv(directory+\"/\"+filename)\n",
    "        \n",
    "        debate_code = str(filename.split(\".\")[0])\n",
    "                \n",
    "        # use only last names (eg SEN. OBAMA -> OBAMA)\n",
    "        who_talks = []\n",
    "        for name in this_file[\"speaker\"]:\n",
    "            try:\n",
    "                who_talks.append(name.split(\" \")[-1].upper().replace(\".\",\"\"))\n",
    "            except:\n",
    "                who_talks.append(np.nan)\n",
    "        this_file[\"speaker\"] = who_talks\n",
    "        \n",
    "        two_cols = this_file.reset_index(drop = True)\n",
    "                \n",
    "        # make new row for each new sentence   \n",
    "        row_num = 0\n",
    "        for row in two_cols[\"speaker\"]:\n",
    "            two_cols[\"speaker\"][row_num] = str(two_cols[\"speaker\"][row_num]).replace('\\n', ' ')\n",
    "            row_num+=1\n",
    "            \n",
    "        row_num = 0\n",
    "        position = []\n",
    "        new_data = []\n",
    "        \n",
    "        try:\n",
    "            for x in two_cols[\"speech\"]:\n",
    "                speaker = two_cols[\"speaker\"][row_num]\n",
    "\n",
    "                s = sent_tokenize(x)\n",
    "                if len(s)>1:\n",
    "                    position.append(row_num)\n",
    "                    for sent in s:\n",
    "                        new_data.append([speaker,sent])\n",
    "\n",
    "                row_num+=1\n",
    "\n",
    "            new_sent = pd.DataFrame.from_records(new_data)\n",
    "            new_sent.columns = [\"speaker\",\"speech\"]\n",
    "            dropped_para = two_cols.drop(position)\n",
    "\n",
    "            sent_only = dropped_para.append(new_sent, ignore_index=False)\n",
    "        except:\n",
    "            sent_only = two_cols\n",
    "        \n",
    "        sent_only = sent_only.reset_index(drop=True)\n",
    "        \n",
    "        # removing special characters \n",
    "        for x in sent_only[\"speech\"]:\n",
    "            try:\n",
    "                t = re.sub('[^a-zA-Z]+', ' ', x) \n",
    "                word_tokens = word_tokenize(t) \n",
    "                filtered_sentence = [w.lower() for w in word_tokens if not w in stop_words] \n",
    "                new_sent = \" \".join(filtered_sentence)\n",
    "                sent_only = sent_only.replace(x,new_sent)\n",
    "            except:\n",
    "                pass\n",
    "        \n",
    "        # make lower case\n",
    "        replaced = []\n",
    "        for row in sent_only[\"speech\"]:\n",
    "            sentence = []\n",
    "            row = str(row)\n",
    "            for word in row.split(\" \"):\n",
    "                sentence.append(word.lower())\n",
    "            as_string = \" \".join(sentence)\n",
    "            replaced.append(as_string)\n",
    "        sent_only[\"speech\"] = replaced  \n",
    "        \n",
    "        # lemmatize words\n",
    "        replaced = []\n",
    "        for row in sent_only[\"speech\"]:\n",
    "            sentence = []\n",
    "            row = str(row)\n",
    "            for word in row.split(\" \"):\n",
    "                sentence.append(lemmatizer.lemmatize(word))\n",
    "            as_string = \" \".join(sentence)\n",
    "            replaced.append(as_string)\n",
    "        sent_only[\"speech\"] = replaced         \n",
    "        \n",
    "        # stem words\n",
    "        replaced = []\n",
    "        for row in sent_only[\"speech\"]:\n",
    "            sentence = []\n",
    "            row = str(row)\n",
    "            for word in row.split(\" \"):\n",
    "                if len(word) > 4:\n",
    "                    sentence.append(stemmer.stem(word))\n",
    "            as_string = \" \".join(sentence)\n",
    "            replaced.append(as_string)\n",
    "        sent_only[\"speech\"] = replaced         \n",
    "        \n",
    "        # remove stop words\n",
    "        replaced = []\n",
    "        for row in sent_only[\"speech\"]:\n",
    "            sentence = []\n",
    "            row = str(row)\n",
    "            for word in row.split(\" \"):\n",
    "                if word not in stop_words:\n",
    "                    sentence.append(word)\n",
    "            as_string = \" \".join(sentence)\n",
    "            replaced.append(as_string)\n",
    "        sent_only[\"speech\"] = replaced\n",
    "        \n",
    "        \n",
    "        # remove speech which is only 2 words or less\n",
    "        remove = []\n",
    "        pos = 0\n",
    "        for row in sent_only[\"speech\"]:\n",
    "            sentence = []\n",
    "            row = str(row)\n",
    "            for word in row.split(\" \"):\n",
    "                sentence.append(word)\n",
    "            if len(sentence) < 3:\n",
    "                remove.append(pos)\n",
    "            pos+=1\n",
    "        \n",
    "        this_file = sent_only.drop(remove)\n",
    "        \n",
    "        # keep only speaker and speech columns\n",
    "        this_file = this_file[[\"speaker\",\"speech\"]]\n",
    "        \n",
    "        # generate new CSV for each debate\n",
    "        this_file = this_file.reset_index(drop=True)\n",
    "        new_name = \"debate_data/debates_clean/\" + str(filename)\n",
    "        this_file.to_csv(new_name,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bit more preprocessing required following visual inspection of data\n",
    "\n",
    "directory = \"debate_data/debates_clean\"\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "           \n",
    "    if filename.endswith(\".csv\"):\n",
    "\n",
    "        data = pd.read_csv(directory+\"/\"+filename)\n",
    "\n",
    "        # remove rows with no speech in them\n",
    "        data = data[pd.notnull(data[\"speech\"])]\n",
    "        data = data.reset_index(drop=True)\n",
    "\n",
    "        # update CSV for each debate\n",
    "        new_name = \"debate_data/debates_clean/\" + str(filename)\n",
    "        data.to_csv(new_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some speakers are called \"PRESIDENT\", rename them\n",
    "\n",
    "winners = pd.read_csv(\"debate_data/election_cycle_winners.csv\")\n",
    "\n",
    "directory = \"debate_data/debates_clean\"\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    \n",
    "    if filename.endswith(\".csv\"):\n",
    "        \n",
    "        this_file = pd.read_csv(directory+\"/\"+filename)\n",
    "        \n",
    "        all_names = []\n",
    "        for row in this_file[\"speaker\"]:\n",
    "            all_names.append(row)\n",
    "        all_names = list(set(all_names))\n",
    "        \n",
    "        if \"PRESIDENT\" in all_names:\n",
    "\n",
    "            debate_code = str(filename.split(\".\")[0])\n",
    "\n",
    "            election_year = int(overview.loc[overview[\"coder\"] == debate_code, \"election_year\"].iloc[0]) - 4\n",
    "\n",
    "            pres_name = winners.loc[winners[\"election_year\"] == election_year, \"G\"].iloc[0]\n",
    "\n",
    "            count1 = 0\n",
    "            for row in this_file[\"speaker\"]:\n",
    "                if row == \"PRESIDENT\":\n",
    "                    this_file[\"speaker\"][count1] = pres_name.upper()\n",
    "                count1+=1\n",
    "\n",
    "            new_file = this_file\n",
    "            new_name = \"debate_data/debates_clean/\" + filename\n",
    "            new_file.to_csv(new_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to join subsequent speakers if the same\n",
    "\n",
    "def join_speakers(data):\n",
    "    sets = []\n",
    "    count1 = 1\n",
    "\n",
    "    for x in data[\"speaker\"]:\n",
    "        group = [count1-1]\n",
    "        count2 = count1\n",
    "        \n",
    "        while count2 < len(data[\"speaker\"]) and x == data[\"speaker\"][count2]:\n",
    "            group.append(count2)\n",
    "            count2+=1\n",
    "        sets.append(group)\n",
    "        \n",
    "        count1+=1\n",
    "    to_drop = []\n",
    "    for x in list(range(0,len(sets))):\n",
    "        try:\n",
    "            if sets[x][1] == sets[x+1][0]:\n",
    "                to_drop.append(x+1)\n",
    "        except:\n",
    "            pass\n",
    "    sets = [i for j, i in enumerate(sets) if j not in to_drop]\n",
    "    for x in sets:\n",
    "        if len(x)>1:\n",
    "            for i in x[1:]:\n",
    "                data[\"speech\"][x[0]] = data[\"speech\"][x[0]] + \". \" + data[\"speech\"][i]\n",
    "    to_keep = list(x[0] for x in sets)\n",
    "    data = data.ix[to_keep].reset_index(drop=True)\n",
    "    \n",
    "    return data\n",
    "\n",
    "directory = \"debate_data/debates_clean\"\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "        \n",
    "    if filename.endswith(\".csv\"):\n",
    "        \n",
    "        # reading file\n",
    "        data = pd.read_csv(directory+\"/\"+filename)\n",
    "        debate_code = str(filename.split(\".\")[0])\n",
    "                \n",
    "        # use only last names (eg SEN. OBAMA -> OBAMA)\n",
    "        who_talks = []\n",
    "        for name in data[\"speaker\"]:\n",
    "            try:\n",
    "                who_talks.append(name.split(\" \")[-1].upper().replace(\".\",\"\"))\n",
    "            except:\n",
    "                who_talks.append(np.nan)\n",
    "        data[\"speaker\"] = who_talks\n",
    "\n",
    "        # join sentences when speaker is continously talking\n",
    "        data = join_speakers(data).reset_index(drop=True)\n",
    "        \n",
    "        # check, prints if there is subsequent naming\n",
    "        check = list(range(0,len(data[\"speaker\"])-1))\n",
    "        for x in check:\n",
    "            try:\n",
    "                if data[\"speaker\"][x] == data[\"speaker\"][x+1]:\n",
    "                    print(data[\"speaker\"][x], data[\"speaker\"][x+1])\n",
    "            except:\n",
    "                print(len(data[\"speaker\"]),x, \"*\")\n",
    "                \n",
    "        # update CSV for each debate\n",
    "        new_name = \"debate_data/debates_clean/\" + str(filename)\n",
    "        data.to_csv(new_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split files to create moderator and participant CSVs\n",
    "\n",
    "directory = \"debate_data/debates_clean\"\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "        \n",
    "    if filename.endswith(\".csv\"):\n",
    "        \n",
    "        # reading file\n",
    "        this_file = pd.read_csv(directory+\"/\"+filename)\n",
    "        \n",
    "        debate_code = str(filename.split(\".\")[0])\n",
    "        \n",
    "        # drop people who speak 3 times or less (indicates audience questions)\n",
    "        extra_names = []\n",
    "        try:\n",
    "            filt_data = this_file[this_file.groupby(\"speaker\").speaker.transform(\"count\")<4]\n",
    "            for row in filt_data[\"speaker\"]:\n",
    "                extra_names.append(row)            \n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "        # moderators and audience comments                    \n",
    "        additional_to_remove = [\"(UNKNOWN)\",\"ANNOUNCER\", \"AUDIENCE\",\"PROTESTER\",\"STUDENT\",\"SAWYER\",\n",
    "                                \"Q\",\"Question\", \"QUESTIONER\",\"APPLAUSE\",\"MODERATOR\",\"Panelists\",\n",
    "                                \"OTHERS\",\"OTHER\",\"cartoon\",\"anchor\",\"CANDIDATE\",\"LAUGHTER\",\"Q.\",\n",
    "                                \"(LAUGHTER)\",\"LAUGHTER\",\"MALE\",\"female\",\"[\",\"]\",\"(\",\")\",\"THIS\",\n",
    "                               \"happened\",\"George\",\"is\",\"ABC\",\"ALBIN\",\"bachman\",\"bagley\",\"berger\",\n",
    "                               \"bream\",\"brooks\",\"Brownstein\",\"Cameron\",\"carolina\",\"cater\",\"Cavuto\",\n",
    "                               \"chancellor\",\"clear\",\"correspondent\",\"Cronkite\",\"Crowley\",\"demint\",\n",
    "                               \"dinan\",\"DiStaso\",\"do\",\"drummond\",\"economy\",\"epperson\",\"fremd\",\"LEVESQUE\",\n",
    "                               \"Goldman\",\"goler\",\"TRUE\",\"You\",\"greenfield\",\"Griffith\",\"hall\",\"Harwood\",\n",
    "                               \"hewitt\",\"hiller\",\"Iowa\",\"Jennings\",\"johns\",\"Karl\",\"king\",\"lemon\",\"Liesman\",\n",
    "                                \"like\",\"louis\",\"maddow\",\"mcgee\",\"mcmanus\",\"Mears\",\"Morales\",\"morgan\",\n",
    "                                \"navarrette\",\"news\",\"niven\",\"novins\",\"obradovich\",\"one\",\"spivak\",\"vancour\",\n",
    "                                \"warren\",\"system\",\"shaw\",\"roe\",\"too\",\"truth\",\"votes\",\"register\",\"roberts\",\n",
    "                                \"rogers\",\"russert\",\"seib\",\"Spradling\",\"three\",\"vaughn\",\"Wallace\",\"Wickham\",\n",
    "                                \"WORKER)\",\"WORKER\",\"yepsen\",\"pelley\",\"reinhard\",\"STEPHANOPOULOS\",\n",
    "                                \"unknown\",\"york\",\"raddatz\",\"right\",\"santelli\",\"Strassel\",\"tumulty\",\"wallace\",\n",
    "                               \"Albin\",\"Bachman\",\"Cameron\",\"Cavuto\",\"Cramer\",\"Crowley\",\"Cummings\",\n",
    "                               \"Demint\",\"Dinan\",\"Distaso\",\"Evans\",\"Fahey\",\"Goldman\",\"Goler\",\"Griffith\",\n",
    "                               \"Ham\",\"Harwood\",\"Hewitt\",\"Hiller\",\"Jennings\",\"King\",\"Lemon\",\"Liesman\",\n",
    "                                \"Lopez\", \"Louis\",\"Maddow\",\"Mcelveen\",\"McManus\",\"Morales\",\"Novins\",\n",
    "                                \"O'brien\",\"Pelley\",\"Raddatz\",\"Reinhard\",\"Russert\",\"Seib\",\"Shaw\",\"LOPEZ\"\n",
    "                                \"Spradling\",\"Stanton\",\"Stephanopoulos\",\"Strassel\",\"Tumulty\",\"UNKNOWN\",\n",
    "                                \"Vaughn\",\"Wallace\",\"Williams\",\"Woodruff\",\"Yepsen\",\"York\", \"MOYERS\",\"Fahey\",\n",
    "                                \"KERR\",\"GOODSON\",\"CRAMER\",\"VANOCUR\",\"TASH\",\"FAHEY\",\"JACKSON\",\"TRULUCK\",\n",
    "                                \"LOPEZ\",\"LEVESQUE\",\"university\",\"(?)\",\"?\",\")\",'NIXON\"',\"ARRARÃS\",\"LEHRER\"]\n",
    "            \n",
    "            \n",
    "        row_num = overview[\"coder\"][overview[\"coder\"] == debate_code].index\n",
    "        \n",
    "        for person in overview[\"moderator\"][row_num]:\n",
    "            for mod in person:\n",
    "                additional_to_remove.append(mod)\n",
    "                \n",
    "        exclude = [\"Bush\",\"Johnson\",\"JACKSON\",\"HUNTER\",\"ROMNEY\",\"Anderson\",\"FIORINA\",\"PERRY\",\"johnson\",\\\n",
    "                         \"REAGAN\",\"clark\",\"sharpton\",\"dukakis\",\"Ford\"]        \n",
    "        exclude = list(set(exclude + [x.lower() for x in exclude] + [x.upper() for x in exclude] \n",
    "                           + [x.capitalize() for x in exclude]))\n",
    "        \n",
    "        mods = additional_to_remove + extra_names\n",
    "\n",
    "        mods = list(set(mods + [x.lower() for x in mods] + [x.upper() for x in mods] \n",
    "                        + [x.capitalize() for x in mods]))\n",
    "        \n",
    "        \n",
    "        non_parts = [x for x in mods if x not in exclude]\n",
    "\n",
    "        pos = []\n",
    "        \n",
    "        count = 0\n",
    "        for x in this_file[\"speaker\"]:\n",
    "            if x in non_parts:\n",
    "                pos.append(count)\n",
    "            count+=1\n",
    "        \n",
    "        # pos is the index position of moderators\n",
    "                \n",
    "        participant_comments = this_file.drop(pos)\n",
    "        participant_comments = participant_comments.dropna(axis=0, subset=['speaker'])\n",
    "        participant_comments = participant_comments.reset_index(drop = True)\n",
    "        participant_comments = join_speakers(participant_comments)\n",
    "        part_name = \"debate_data/debates_part/\" + str(filename)\n",
    "\n",
    "        moderator_comments = this_file.iloc[pos,:]\n",
    "        moderator_comments = moderator_comments.dropna(axis=0, subset=['speaker'])\n",
    "        moderator_comments = moderator_comments.reset_index(drop = True)\n",
    "        moderator_comments = join_speakers(moderator_comments)\n",
    "        mod_name = \"debate_data/debates_mod/\" + str(filename)\n",
    "        \n",
    "        participant_comments.to_csv(part_name, index=False)\n",
    "        moderator_comments.to_csv(mod_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename participants in each debate as NAME+YYYY where YYYY stands for election year\n",
    "\n",
    "directory = \"debate_data/debates_part\"\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "        \n",
    "    if filename.endswith(\".csv\"):\n",
    "        \n",
    "        # reading file\n",
    "        this_file = pd.read_csv(directory+\"/\"+filename)\n",
    "        \n",
    "        debate_code = str(filename.split(\".\")[0])\n",
    "\n",
    "        year_code = int(debate_code[1:5])\n",
    "        \n",
    "        if year_code % 4 != 0:\n",
    "            year_code = year_code+1\n",
    "        \n",
    "        new_speaker = []\n",
    "        for row in this_file[\"speaker\"]:\n",
    "            name = str(row) + str(year_code)\n",
    "            new_speaker.append(name)\n",
    "            \n",
    "        this_file[\"speaker\"] = new_speaker\n",
    "        \n",
    "        part_name = \"debate_data/debates_part/\" + str(filename)\n",
    "        this_file.to_csv(part_name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add list of participants to the overview file\n",
    "\n",
    "overview[\"participants\"] = np.nan\n",
    "\n",
    "directory = \"debate_data/debates_part\"\n",
    "\n",
    "participants_ordered = []\n",
    "for filename in os.listdir(directory):\n",
    "\n",
    "    if filename.endswith(\".csv\"):\n",
    "                \n",
    "        data = pd.read_csv(directory+\"/\"+filename)\n",
    "        \n",
    "        debate_code = str(filename.split(\".\")[0])\n",
    "        \n",
    "        participants = []\n",
    "        for row in data[\"speaker\"]:\n",
    "            participants.append(row)\n",
    "        participants = list(set(participants))\n",
    "        \n",
    "        location = overview[\"coder\"][overview[\"coder\"] == debate_code].index.tolist()\n",
    "        \n",
    "        for i in location:\n",
    "            location = int(i)\n",
    "        \n",
    "        overview[\"participants\"][location] = participants\n",
    "        \n",
    "overview.to_csv(\"debate_data/data_overviews/overview_1.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create candidate data CSV \n",
    "\n",
    "candidate_data = pd.DataFrame(columns = [\"participants\",\"year\",\"part_code\"])\n",
    "\n",
    "all_p = []\n",
    "for row in overview[\"participants\"]:\n",
    "    for x in row:\n",
    "        all_p.append(x)\n",
    "all_p = list(set(all_p))\n",
    "\n",
    "name = []\n",
    "year = []\n",
    "part_code = []\n",
    "for x in all_p:\n",
    "    name.append(x[:-4])\n",
    "    year.append(x[-4:])\n",
    "    part_code.append(x)\n",
    "candidate_data[\"participants\"] = name\n",
    "candidate_data[\"year\"] = year\n",
    "candidate_data[\"part_code\"] = part_code\n",
    "\n",
    "candidate_data.to_csv(\"debate_data/data_overviews/candidate_data_1.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add background data to candidates\n",
    "\n",
    "candidate_background = pd.read_csv(\"debate_data/data_overviews/candidate_background.csv\")\n",
    "\n",
    "candidate_data[\"election_age\"] = np.nan\n",
    "candidate_data[\"gender\"] = np.nan\n",
    "candidate_data[\"party\"] = np.nan\n",
    "candidate_data[\"general\"] = np.nan\n",
    "candidate_data[\"affiliation\"] = np.nan\n",
    "\n",
    "non_candidates_to_drop = []\n",
    "\n",
    "for x in candidate_data[\"part_code\"]:\n",
    "    \n",
    "    location1 = candidate_background[\"part_code\"][candidate_background[\"part_code\"] == x].index.tolist()\n",
    "    for i in location1:\n",
    "            location1 = int(i)\n",
    "    \n",
    "    location2 = candidate_data[\"part_code\"][candidate_data[\"part_code\"] == x].index.tolist()\n",
    "    for i in location2:\n",
    "            location2 = int(i)\n",
    "    try:        \n",
    "        candidate_data[\"election_age\"][location2] = candidate_background[\"election_age\"][location1]\n",
    "        candidate_data[\"gender\"][location2] = candidate_background[\"gender\"][location1]\n",
    "        candidate_data[\"party\"][location2] = candidate_background[\"party\"][location1]\n",
    "        candidate_data[\"general\"][location2] = candidate_background[\"general\"][location1]\n",
    "        candidate_data[\"affiliation\"][location2] = candidate_background[\"affiliation\"][location1]\n",
    "    except:\n",
    "        non_candidates_to_drop.append(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_data = candidate_data[~candidate_data['part_code'].isin(non_candidates_to_drop)].reset_index(drop=True)\n",
    "        \n",
    "candidate_data[\"n_debates\"] = np.nan\n",
    "\n",
    "all_parts = []\n",
    "\n",
    "for x in overview[\"participants\"]:\n",
    "    for i in x:\n",
    "        all_parts.append(i)\n",
    "\n",
    "for x in all_parts:\n",
    "    location1 = candidate_data[\"part_code\"][candidate_data[\"part_code\"] == x].index.tolist()\n",
    "    for i in location1:\n",
    "            location1 = int(i)\n",
    "    candidate_data[\"n_debates\"][i] = all_parts.count(x)\n",
    "    \n",
    "count = 0\n",
    "for x in overview[\"participants\"]:\n",
    "    parts = []\n",
    "    for i in x:\n",
    "        parts.append(i)\n",
    "    updated_parts = [x for x in parts if x not in non_candidates_to_drop]\n",
    "    overview[\"participants\"][count] = updated_parts\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create file for each speaker\n",
    "speakers = pd.read_csv(\"debate_data/data_overviews/candidate_data_1.csv\")       \n",
    "        \n",
    "for row in speakers[\"part_code\"]:\n",
    "    new_file = pd.DataFrame(columns = [\"speaker\",\"speech\"])\n",
    "    new_name = \"debate_data/speakers/\" + str(row) +\".csv\"\n",
    "    new_file.to_csv(new_name,index=False)\n",
    "    \n",
    "# join all the debates to one csv\n",
    "all_part_debates = pd.DataFrame(columns = [\"speaker\",\"speech\"])\n",
    "\n",
    "directory = \"debate_data/debates_part\"\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        data = pd.read_csv(directory+\"/\"+filename)\n",
    "        all_part_debates = pd.concat([all_part_debates, data], ignore_index=True)         \n",
    "\n",
    "# fill speaker files\n",
    "for coder in candidate_data[\"part_code\"]:\n",
    "\n",
    "    new_file = all_part_debates.loc[all_part_debates.speaker == coder]\n",
    "    new_file = new_file.reset_index(drop = True)\n",
    "    new_name = \"debate_data/speakers/\" + str(coder) +\".csv\"\n",
    "    new_file.to_csv(new_name,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_data[\"total_words\"] = np.nan\n",
    "candidate_data[\"unique_words\"] = np.nan\n",
    "\n",
    "directory = \"debate_data/speakers\"\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "\n",
    "    if filename.endswith(\".csv\"):\n",
    "                \n",
    "        data = pd.read_csv(directory+\"/\"+filename)\n",
    "        \n",
    "        debate_code = str(filename.split(\".\")[0])\n",
    "        \n",
    "        words = []\n",
    "\n",
    "        for x in data[\"speech\"]:\n",
    "            for i in x.split(\" \"):\n",
    "                words.append(i)\n",
    "                \n",
    "        loc = candidate_data[\"part_code\"][candidate_data[\"part_code\"] == debate_code].index\n",
    "        n_words = len(words)\n",
    "        candidate_data[\"total_words\"][loc] = n_words\n",
    "        candidate_data[\"unique_words\"][loc] = len(set(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>participants</th>\n",
       "      <th>year</th>\n",
       "      <th>part_code</th>\n",
       "      <th>election_age</th>\n",
       "      <th>gender</th>\n",
       "      <th>party</th>\n",
       "      <th>general</th>\n",
       "      <th>affiliation</th>\n",
       "      <th>n_debates</th>\n",
       "      <th>total_words</th>\n",
       "      <th>unique_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>QUAYLE</td>\n",
       "      <td>1988</td>\n",
       "      <td>QUAYLE1988</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>rep</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1391.0</td>\n",
       "      <td>644.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>BENTSEN</td>\n",
       "      <td>1988</td>\n",
       "      <td>BENTSEN1988</td>\n",
       "      <td>67.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>dem</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1417.0</td>\n",
       "      <td>675.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>O'MALLEY</td>\n",
       "      <td>2016</td>\n",
       "      <td>O'MALLEY2016</td>\n",
       "      <td>53.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>dem</td>\n",
       "      <td>5.0</td>\n",
       "      <td>4703.0</td>\n",
       "      <td>1478.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>JOHNSON</td>\n",
       "      <td>2012</td>\n",
       "      <td>JOHNSON2012</td>\n",
       "      <td>59.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>rep</td>\n",
       "      <td>1.0</td>\n",
       "      <td>158.0</td>\n",
       "      <td>113.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>GORE</td>\n",
       "      <td>1996</td>\n",
       "      <td>GORE1996</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>dem</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1633.0</td>\n",
       "      <td>755.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>DUKAKIS</td>\n",
       "      <td>1988</td>\n",
       "      <td>DUKAKIS1988</td>\n",
       "      <td>55.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>dem</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3434.0</td>\n",
       "      <td>1147.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>BRADLEY</td>\n",
       "      <td>2000</td>\n",
       "      <td>BRADLEY2000</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>dem</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8145.0</td>\n",
       "      <td>2024.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>MCCAIN</td>\n",
       "      <td>2008</td>\n",
       "      <td>MCCAIN2008</td>\n",
       "      <td>82.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>rep</td>\n",
       "      <td>18.0</td>\n",
       "      <td>12987.0</td>\n",
       "      <td>2685.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>LIEBERMAN</td>\n",
       "      <td>2004</td>\n",
       "      <td>LIEBERMAN2004</td>\n",
       "      <td>62.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>dem</td>\n",
       "      <td>2.0</td>\n",
       "      <td>746.0</td>\n",
       "      <td>443.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>CLINTON</td>\n",
       "      <td>2008</td>\n",
       "      <td>CLINTON2008</td>\n",
       "      <td>61.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>dem</td>\n",
       "      <td>19.0</td>\n",
       "      <td>16223.0</td>\n",
       "      <td>3072.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    participants  year      part_code  election_age  gender  party  general  \\\n",
       "0         QUAYLE  1988     QUAYLE1988          41.0     0.0    1.0      1.0   \n",
       "1        BENTSEN  1988    BENTSEN1988          67.0     0.0    1.0      0.0   \n",
       "2       O'MALLEY  2016   O'MALLEY2016          53.0     0.0    0.0      0.0   \n",
       "3        JOHNSON  2012    JOHNSON2012          59.0     0.0    0.0      0.0   \n",
       "4           GORE  1996       GORE1996          48.0     0.0    1.0      1.0   \n",
       "..           ...   ...            ...           ...     ...    ...      ...   \n",
       "96       DUKAKIS  1988    DUKAKIS1988          55.0     0.0    1.0      0.0   \n",
       "97       BRADLEY  2000    BRADLEY2000          57.0     0.0    0.0      0.0   \n",
       "98        MCCAIN  2008     MCCAIN2008          82.0     0.0    1.0      0.0   \n",
       "99     LIEBERMAN  2004  LIEBERMAN2004          62.0     0.0    0.0      0.0   \n",
       "100      CLINTON  2008    CLINTON2008          61.0     1.0    0.0      0.0   \n",
       "\n",
       "    affiliation  n_debates  total_words  unique_words  \n",
       "0           rep        1.0       1391.0         644.0  \n",
       "1           dem        1.0       1417.0         675.0  \n",
       "2           dem        5.0       4703.0        1478.0  \n",
       "3           rep        1.0        158.0         113.0  \n",
       "4           dem        1.0       1633.0         755.0  \n",
       "..          ...        ...          ...           ...  \n",
       "96          dem        2.0       3434.0        1147.0  \n",
       "97          dem        8.0       8145.0        2024.0  \n",
       "98          rep       18.0      12987.0        2685.0  \n",
       "99          dem        2.0        746.0         443.0  \n",
       "100         dem       19.0      16223.0        3072.0  \n",
       "\n",
       "[101 rows x 11 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# update candidate_data_1\n",
    "candidate_data.to_csv(\"debate_data/data_overviews/candidate_data_1.csv\",index=False)\n",
    "candidate_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
