{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import threading, requests\n",
    "from queue import Queue\n",
    "from time import sleep\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class __data_mining__():\n",
    "    def __init__(self,inp_url):\n",
    "        self.inp_url = inp_url\n",
    "        self.out_list = []\n",
    "        self.d_frame = pd.DataFrame()\n",
    "        self.col_headers = ['full date of debate','year of debate','classification if speech is : General, Primary-D (democratic), Primary-R (republican)','link/URL to debate','title of debate','participant names as a list [name 1, name 2 etc]','moderator name','full transcript of debate','text as a dictionary{speaker_name: (first speech, second speech) etc }','text as a list of lists [ [speaker 1, text1] etc ]']\n",
    "    def data_mine(self,row):\n",
    "        # initialize text variable for each row\n",
    "        rows_text = ''\n",
    "        try:\n",
    "            # read date and title. Also remove unwanted text\n",
    "            rows_text = row.text.strip().replace('\\n','')\n",
    "            # split with seperator to read date and speach title seperately\n",
    "            full_date_debate,title_of_debate = rows_text.split('\\xa0')[0],rows_text.split('\\xa0')[1]\n",
    "            # classification of speech - General, Primary-D (democratic), Primary-R (republican)\n",
    "            if 'Democratic' in title_of_debate:\n",
    "                speach_type = 'Primary-D (democratic)'\n",
    "            elif 'Republican' in title_of_debate:\n",
    "                speach_type = 'Primary-R (republican)'\n",
    "            else:\n",
    "                speach_type = 'General'\n",
    "            # read link for each speach, if available\n",
    "            speach_link = row.find('a')['href']\n",
    "            print(speach_link)\n",
    "            # get request on speach link to read content, and scrape required data\n",
    "            speach_resp = requests.get(speach_link)\n",
    "            speach_soup = BeautifulSoup(speach_resp.text, 'lxml')\n",
    "            page = speach_soup.find(\"div\", attrs={\"class\":\"field-docs-content\"})\n",
    "            # read participant names as a list [name 1, name 2 etc]\n",
    "            # for handling single or multiple participants\n",
    "            if ';' in page.find_all('p')[0].text:\n",
    "                participant_list = page.find_all('p')[0].text.split(';')[:-1]\n",
    "                participant_list = [each.replace('\\n','') for each in participant_list]\n",
    "            else:\n",
    "                participant_list = [page.find_all('p')[0].text]\n",
    "            # read moderator name, as there are more then one moderator hence providing \n",
    "            # list of moderator names\n",
    "            # for handling single or multiple moderators\n",
    "            if ';' in page.find_all('p')[1].text:\n",
    "                moderator_list = page.find_all('p')[1].text.split(';')[:-1]\n",
    "                moderator_list = [each.replace('\\n','') for each in moderator_list]\n",
    "            else:\n",
    "                moderator_list = [page.find_all('p')[1].text]\n",
    "            # read full transcript of debate, text as a dictionary, speaker name\n",
    "            # text as a list of lists\n",
    "            full_transript = ''\n",
    "            text_as_dict = {}\n",
    "            text_as_list = []\n",
    "            full_transript_list = page.find_all('p')\n",
    "            # exclude first two paragraphs as they are for participants and moderators\n",
    "            for i in range(2,len(full_transript_list)):\n",
    "                # handling multiple paragraphs of debate for each speaker\n",
    "                try:\n",
    "                    each_transript = full_transript_list[i]\n",
    "                    speaker_name = each_transript.strong.text.strip(':')\n",
    "                    full_transript = full_transript+' '+each_transript.text.strip()\n",
    "                    # text as a dictionary {speaker_name: (first speech, second speech) etc }\n",
    "                    if speaker_name in text_as_dict.keys():\n",
    "                        text_as_dict[speaker_name] = text_as_dict[speaker_name].append(each_transript.text.split(':')[1])\n",
    "                    else:\n",
    "                        text_as_dict[speaker_name] = [each_transript.text.split(':')[1]]\n",
    "                    # text as a list of lists\n",
    "                    text_as_list.append([speaker_name,each_transript.text.split(':')[1]])\n",
    "                except Exception as e:\n",
    "                    # nested excepttional handler to different data patterns\n",
    "                    try:\n",
    "                        text_as_list.append([speaker_name,each_transript.text.strip()])\n",
    "                        text_as_dict[speaker_name] = [each_transript.text.strip()]\n",
    "                    except Exception as e:\n",
    "                        pass\n",
    "            self.out_list.append([full_date_debate,full_date_debate.split(',')[1].strip(),speach_type,speach_link,title_of_debate,participant_list,moderator_list,full_transript,text_as_dict,text_as_list])\n",
    "        except Exception as e:\n",
    "            # print(row.text.strip())\n",
    "            pass\n",
    "    def start_scrape(self):\n",
    "        # perform get request on input link\n",
    "        resp = requests.get(self.inp_url)\n",
    "        # create soup object for page text\n",
    "        soup = BeautifulSoup(resp.text, 'lxml')\n",
    "        table = soup.find(\"div\", attrs={\"class\":\"field-body\"})\n",
    "        # read all dates and speach types from page\n",
    "        rows = table.find_all('tr')\n",
    "        # perform multithreading to process multiple debate link for each thread\n",
    "        def threader():\n",
    "            while True:\n",
    "                # gets an worker from the queue\n",
    "                x = q.get()\n",
    "                # Run the example job with the avail worker in queue (thread)\n",
    "                self.data_mine(x)\n",
    "                # completed with the job\n",
    "                q.task_done()\n",
    "        q = Queue()\n",
    "        # initialize 40 threads for handling debate links\n",
    "        print('start processing')\n",
    "        for x in range(40):\n",
    "            t = threading.Thread(target=threader)\n",
    "            # classifying as a daemon, so they will die when the main dies\n",
    "            t.daemon = True\n",
    "            sleep(0.3)\n",
    "            # begins, must come after daemon definition\n",
    "            t.start()\n",
    "        # Jobs assigned.\n",
    "        # iterate through each speach to read content\n",
    "        for row in rows:\n",
    "            # self.data_mine(row)\n",
    "            q.put(row)\n",
    "        q.join ()\n",
    "        # create dataframe from the output list\n",
    "        full_dict = {k: [x[i] for x in self.out_list] for i, k in enumerate(self.col_headers)}\n",
    "        self.d_frame = pd.DataFrame.from_dict(full_dict)\n",
    "        # write dataframe to csv file\n",
    "        f_paht_csv = r'Scraping_CSV.csv'\n",
    "        f_paht_xls = r'Scraping_Xls.xlsx'\n",
    "        # self.d_frame.to_csv(f_paht_csv, index=None, header=True)\n",
    "        writer = pd.ExcelWriter(f_paht_xls, engine='xlsxwriter', options={'strings_to_urls': False})\n",
    "        self.d_frame.to_excel(writer, index=False, columns=self.col_headers)\n",
    "        writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create object for class and call main method\n",
    "class_obj = __data_mining__('https://www.presidency.ucsb.edu/documents/presidential-documents-archive-guidebook/presidential-candidates-debates-1960-2016')\n",
    "class_obj.start_scrape()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
